{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duong4595/ib9cw0-text-analytics-23-24/blob/main/Lecture_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis"
      ],
      "metadata": {
        "id": "bQm73Ws5eeUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "W0lpjK_7HDk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text  # Remove HTML tags\n",
        "    text = re.sub(r'[\\W]', ' ', text)  # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Example usage\n",
        "dirty_text = \"This is an <html>example</html> text with ðŸ˜Š\"\n",
        "cleaned_text = clean_text(dirty_text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "id": "RGaOMYAOQly9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Methods"
      ],
      "metadata": {
        "id": "oY7lHGZx-PpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AFINN\n",
        "\n",
        "The AFINN lexicon is used primarily in a sentiment analysis technique known as the lexicon-based approach. This approach involves assigning sentiment scores to individual words found in a text and then aggregating these scores to determine the overall sentiment of the text. The AFINN lexicon is particularly suited for this method due to its simplicity and direct mapping of words to numerical sentiment scores.\n",
        "\n",
        "Key Characteristics of the AFINN Lexicon-Based Sentiment Analysis:\n",
        "Scoring: Each word in the AFINN lexicon is assigned a score ranging from -5 to +5, where negative numbers represent negative sentiments and positive numbers represent positive sentiments. The score indicates the intensity of the sentiment.\n",
        "\n",
        "Simplicity: The technique is straightforward to implement because it directly sums up the scores of the words that appear in the text. It does not require complex algorithms or machine learning models.\n",
        "\n",
        "Efficiency: This approach can be very fast, making it suitable for applications that need to process large volumes of text quickly, such as real-time sentiment analysis on social media platforms.\n",
        "\n",
        "Context Ignorance: While efficient and straightforward, the major drawback is that it generally ignores the context in which words are used. This can lead to inaccuracies, especially in texts where the sentiment is conveyed through sarcasm, irony, or context-dependent meanings.\n",
        "\n",
        "AFINN is particularly favored for its ease of use and effectiveness in straightforward applications where context and linguistic subtlety are less critical."
      ],
      "metadata": {
        "id": "ZtPaahtY-8Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install afinn"
      ],
      "metadata": {
        "id": "AnjUUL8x_CsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "afinn = Afinn()\n",
        "\n",
        "# Example usage\n",
        "score = afinn.score('This is excellent!')\n",
        "print(score)  # Positive score\n"
      ],
      "metadata": {
        "id": "hv-Ra_ff_BDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentiWordNet\n",
        "\n",
        "SentiWordNet is utilized in sentiment analysis through a lexicon-based approach similar to AFINN, but it provides a more nuanced analysis by incorporating multiple sentiment scores for words based on their usage in different contexts (synsets). SentiWordNet, an extension of the widely used WordNet database, provides scores for positivity, negativity, and objectivity for each synset (group of synonymous words that share a common meaning).\n",
        "\n",
        "Characteristics of SentiWordNet-Based Sentiment Analysis:\n",
        "Synset-Based Scoring: Each entry (or synset) in WordNet, which can be a word or a phrase, has associated sentiment scores in SentiWordNet:\n",
        "\n",
        "Positivity Score: Indicates how positive a synset is.\n",
        "Negativity Score: Indicates how negative a synset is.\n",
        "Objectivity Score: Indicates how objective, or neutral, a synset is.\n",
        "Contextual Sensitivity: Unlike simpler lexicon-based methods that assign a single sentiment score to a word regardless of usage, SentiWordNet's linkage to WordNet synsets allows it to provide different sentiment scores depending on the context in which a word is used.\n",
        "\n",
        "Sentiment Calculation: The overall sentiment of a text can be calculated by analyzing the sentiment scores of its constituent words based on their relevant synsets, taking into account the wordsâ€™ part of speech and sense.\n",
        "\n",
        "Complexity: This approach is more complex than using a straightforward dictionary like AFINN because it requires determining the correct sense of a word within its context before applying the sentiment scores.\n",
        "\n",
        "Beyond simply classifying sentiments as positive or negative, SentiWordNet allows for measuring the degree of sentiment and objectivity, supporting more detailed analyses.SentiWordNet provides a rich lexical resource for sentiment analysis, especially useful in applications requiring a deep understanding of the lexical semantics of the language."
      ],
      "metadata": {
        "id": "wCiFvSH5BXUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "# Ensure that the necessary resources are downloaded\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "\n",
        "def get_sentiment(word, pos=None):\n",
        "    \"\"\"Get the sentiment scores for the best sense of the word\"\"\"\n",
        "    synsets = wn.synsets(word, pos=pos)\n",
        "    if not synsets:\n",
        "        return None\n",
        "\n",
        "    # Choose the first synset as the most common usage\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "    return swn_synset.pos_score(), swn_synset.neg_score(), swn_synset.obj_score()\n",
        "\n",
        "# Example usage\n",
        "word = 'happy'\n",
        "pos_score, neg_score, obj_score = get_sentiment(word, pos=wn.ADJ)\n",
        "print(f\"Positive score: {pos_score}, Negative score: {neg_score}, Objective score: {obj_score}\")\n"
      ],
      "metadata": {
        "id": "MRQCT_zpBziI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextBlob\n",
        "\n",
        "TextBlob is another popular library for processing textual data in Python. It is particularly known for its simplicity and ease of use, providing a straightforward API for tackling common natural language processing (NLP) tasks, including part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
        "\n",
        "Overview of TextBlob in Sentiment Analysis:\n",
        "\n",
        "TextBlobâ€™s sentiment analysis is built upon the pattern library, which itself is based on a lexicon similar to AFINN but includes an assessment of both polarity and subjectivity:\n",
        "\n",
        "* Polarity: Measures the positivity or negativity of the text. The polarity score is a float within the range [-1.0, 1.0].\n",
        "* Subjectivity: Measures the degree of personal opinion, emotion, or judgment within the text. The subjectivity score is a float within the range [0.0, 1.0].\n",
        "\n",
        "Key Features of TextBlob:\n",
        "\n",
        "* Ease of Use: TextBlob's API is very user-friendly, making it easy to perform complex NLP tasks with only a few lines of code.\n",
        "\n",
        "* Language Support and Translation: TextBlob supports multiple languages for basic NLP tasks and can leverage Google Translate for text translation.\n",
        "\n",
        "TextBlob fits well into the sentiment analysis toolkit as a versatile, easy-to-use option suitable for various texts. TextBlob assesses both the emotional leaning and the objective vs. subjective nature of the text."
      ],
      "metadata": {
        "id": "3jcbuoJK-WPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def analyze_sentiment_textblob(text):\n",
        "    \"\"\"Function to analyze sentiment using TextBlob's built-in sentiment analyzer.\"\"\"\n",
        "    return TextBlob(text).sentiment\n",
        "\n",
        "# Example usage\n",
        "example_text = \"This is a good product!\"\n",
        "sentiment = analyze_sentiment_textblob(example_text)\n",
        "print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")\n"
      ],
      "metadata": {
        "id": "BcwFdvdlP-Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VADER\n",
        "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. It is different from other sentiment analysis tools like AFINN and SentiWordNet in several significant ways:\n",
        "\n",
        "Key Features of VADER:\n",
        "\n",
        "* Specifically Designed for Social Media: VADER was developed with a focus on social media contexts, taking into account the peculiarities and informal language used in tweets, Facebook posts, and other similar platforms.\n",
        "\n",
        "* Rule-Based with a Sentiment Lexicon: VADER combines a sentiment lexicon (a list of lexical features e.g., words, emoji) that are generally labeled according to their semantic orientation as either positive or negative. What makes VADER particularly powerful is that it also incorporates grammatical and syntactical rules to determine sentiment, which helps capture more nuances than typical lexicons.\n",
        "\n",
        "* Handles Slang and Emoticons: It understands modern slang used on social media (e.g., \"sux\", \"lol\") and includes a robust set of emoticons. This feature is crucial for analyzing contemporary sentiment expressions on social media effectively.\n",
        "\n",
        "* Contextual Awareness: VADER not only scores words but also considers context. For example, it intensifies the sentiment if it detects an exclamation mark or diminishes it if it detects words like \"kind of\" or \"sort of\".\n",
        "\n",
        "How VADER Differs from AFINN and SentiWordNet:\n",
        "\n",
        "* AFINN: AFINN provides a list of words rated from -5 to +5. It is straightforward and simple, assigning scores to words without considering context or any modifiers. It does not account for slang, emoticons, or idiomatic phrases often found in social media, making it less effective for such platforms compared to VADER.\n",
        "\n",
        "* SentiWordNet: This is an extension of WordNet which provides scores for positivity, negativity, and objectivity for each WordNet synset (group of synonymous words). While SentiWordNet is more sophisticated than AFINN in handling context due to its association with specific meanings of words, it lacks the built-in rules for handling the dynamics of sentence-level sentiment, modifiers, or contemporary slang and emojis, making it less suitable for on-the-fly social media sentiment analysis compared to VADER."
      ],
      "metadata": {
        "id": "eG6HuiZ_-as4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "print(sid.polarity_scores('This is amazingly good!'))\n"
      ],
      "metadata": {
        "id": "XCnQpZZQ_NF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer based Sentiment Analysis\n",
        "\n",
        "BERT and its variants (like RoBERTa, DistilBERT, etc.) represent a significant shift in how machines understand human languages due to their architecture and training approaches, which allow them to capture the context of every word in a text in relation to all the other words in the sentence, rather than in one-direction at a time.\n",
        "\n",
        "Key Features of BERT for Sentiment Analysis:\n",
        "\n",
        "* Contextual Understanding: BERTâ€™s most significant feature is its ability to consider the full context of a word by looking at the words that come before and after it. This is a considerable advantage for sentiment analysis, especially in handling sentences where the meaning can significantly change based on context or word placement.\n",
        "\n",
        "* Pre-training on Large Corpuses: BERT is pre-trained on a vast corpus of text from the internet, which gives it a broad understanding of language and context before it is even fine-tuned for specific tasks like sentiment analysis.\n",
        "\n",
        "Comparison with Other Sentiment Analysis Tools:\n",
        "\n",
        "* TextBlob & AFINN: These tools use static lexicons or simple rule-based approaches for sentiment analysis. They do not account for the context in which a word appears, which can lead to inaccurate sentiment analysis in more complex sentences. BERT, by contrast, considers the entire sentence structure, which helps in understanding context-dependent meanings.\n",
        "\n",
        "\n",
        "* VADER: VADER is highly tuned for social media and can interpret slang, emojis, and emoticons effectively. While BERT can also be fine-tuned for social media text, it would require specific training data reflecting these nuances. VADER is out-of-the-box ready for social media, whereas BERT would need some adaptation.\n",
        "\n",
        "* SentiWordNet: Unlike SentiWordNet, which provides scores based on word senses, BERT evaluates the sentiment based on the overall sentence semantics, making it much more robust for sentences where multiple word senses are involved.\n",
        "\n",
        "BERTSentiment, leveraging models like BERT, fits into sentiment analysis as a high-performance tool capable of understanding nuanced and context-dependent language, making it superior for applications requiring a deep understanding of text sentiment. It is particularly effective in environments where the context significantly impacts meaning, such as in customer feedback, movie reviews, and other forms of textual analysis where traditional lexicons might fall short. Its use, however, requires more computational resources compared to simpler tools like TextBlob or VADER, and it might be considered overkill for simpler applications."
      ],
      "metadata": {
        "id": "2KKaU8L8Fd_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "id": "TKgK92k_QIo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Analyze sentiment\n",
        "result = sentiment_pipeline(\"I love using transformers for natural language processing!\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "nwskF8qbQNYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complexities of language\n"
      ],
      "metadata": {
        "id": "sc0SCqGFHUzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complex sentiments"
      ],
      "metadata": {
        "id": "H_TW80A_jf4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "from transformers import pipeline\n",
        "nlp = pipeline(\"sentiment-analysis\")\n"
      ],
      "metadata": {
        "id": "ySq1dRoAji2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let us try the following sentences and capture the sentiment.\n",
        "\n",
        "\"The movie isn't really all that great.\"\n",
        "\n",
        "\"Oh great, another rainy day.\"\n",
        "\n",
        " \"Iâ€™m not unhappy with how things turned out.\"\n",
        "\n",
        " \"is this good - more I think about it I do no think so\"\n",
        "\n",
        " \"This camera fails to impress me.\"\n",
        "\n",
        " \"This product is barely functional\"\n",
        "\n",
        " \"What a great car, it did not start the first day.\"\n",
        "\n",
        " \"Trying out Chrome because Firefox keeps crashing\"\n",
        "\n",
        "\"For paintX, one coat can cover the wood color\"\n",
        "\n",
        "\"For paintY, we need three coats to cover the wood color\"\n",
        "\n",
        "\"hello :-) ;-)\""
      ],
      "metadata": {
        "id": "ZNvB3TZm062f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"No, I am good.\"\n",
        "    \"I am no good.\"\n",
        "    \"The movie isn't really all that great.\",\n",
        "    \"Oh great, another rainy day.\",\n",
        "    \"Iâ€™m not unhappy with how things turned out.\",\n",
        "    \"is this good - more I think about it I do no think so\",\n",
        "    \"This camera fails to impress me.\",\n",
        "    \"This product is barely functional.\",\n",
        "    \"What a great car, it did not start the first day.\",\n",
        "    \"Trying out Chrome because Firefox keeps crashing.\",\n",
        "    \"For paintX, one coat can cover the wood color.\",\n",
        "    \"For paintY, we need three coats to cover the wood color.\",\n",
        "    \"hello :-) ;-)\"\n",
        "]"
      ],
      "metadata": {
        "id": "MCCoka6x2P75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List to collect the sentiment data\n",
        "data = []\n",
        "\n",
        "# Iterate through each text to analyze sentiment\n",
        "for text in texts:\n",
        "    # TextBlob sentiment\n",
        "    blob = TextBlob(text)\n",
        "    tb_sentiment = blob.sentiment\n",
        "\n",
        "    # VADER sentiment\n",
        "    vader_sentiment = sid.polarity_scores(text)\n",
        "\n",
        "    # BERT sentiment\n",
        "    bert_result = nlp(text)\n",
        "\n",
        "    # Append results to data list\n",
        "    data.append({\n",
        "        \"Text\": text,\n",
        "        \"TextBlob Sentiment\": tb_sentiment[0],\n",
        "        \"VADER Compound Score\": vader_sentiment['compound'],\n",
        "        \"BERT Sentiment\": bert_result[0]['label'],\n",
        "        \"BERT Score\": bert_result[0]['score'],\n",
        "    })\n",
        "\n",
        "# Create DataFrame from the collected data\n",
        "sentiment_df = pd.DataFrame(data)\n",
        "sentiment_df"
      ],
      "metadata": {
        "id": "AAZgUejZ39p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Slang"
      ],
      "metadata": {
        "id": "YvNZMcRvWSNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the slang lexicon\n",
        "slang_df = pd.read_csv(\"slang_lexicon.csv\")\n",
        "slang_df.head()"
      ],
      "metadata": {
        "id": "RsezYPqmgggZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the DataFrame into a dictionary with lower case keys\n",
        "slang_dict = pd.Series(slang_df['explanation'].values, index=slang_df['slang'].str.lower()).to_dict()\n"
      ],
      "metadata": {
        "id": "yiZfih-HdbPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace slang\n",
        "import re\n",
        "\n",
        "def replace_slang(text, slang_dict):\n",
        "    # Use regex to extract words, keeping them only and converting text to lower case for matching\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Extracts words and converts them to lower case\n",
        "    # Replace each word with the corresponding explanation if it exists in the dictionary\n",
        "    return ' '.join([slang_dict.get(word, word) for word in words])"
      ],
      "metadata": {
        "id": "aV9x1xocddEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text_with_slang = \"BRB, going to the store, LOL!\"\n",
        "cleaned_text = replace_slang(text_with_slang, slang_dict)\n",
        "print(cleaned_text)\n",
        "\n",
        "text_with_slang = \"tbh, icymi i was like ok\"\n",
        "cleaned_text = replace_slang(text_with_slang, slang_dict)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "R907yV-Of_ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "1. Conduct sentiment analysis on `sms_spam.csv`. Assess if the average sentiment of the text is related to whether the message is 'ham' or 'spam'.\n",
        "2. Open the `kickstarter.csv` file, which contains information about various Kickstarter projects, such as their title, description, funding goal, number of backers, and state (successful, failed, canceled, etc.). The file was downloaded from https://webrobots.io/kickstarter-datasets/. Conduct sentiment analysis on the variable `blurb` and assess whether the sentiment is related to `state` which has values successful, failed, and canceled.\n"
      ],
      "metadata": {
        "id": "XBsCExGNUfSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexicons"
      ],
      "metadata": {
        "id": "oonrcXrH5RyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "# Initialize VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Access the lexicon (which is a dictionary of words and their scores)\n",
        "vader_lexicon = sid.lexicon\n",
        "\n",
        "# Print some entries from the lexicon\n",
        "print({word: score for word, score in list(vader_lexicon.items())[:100]})"
      ],
      "metadata": {
        "id": "JtOIgIOQ5UrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the lexicons"
      ],
      "metadata": {
        "id": "FCaJqhCP69TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install afinn"
      ],
      "metadata": {
        "id": "K9chpeoU9mvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AFINN\n",
        "import csv\n",
        "from afinn import Afinn\n",
        "\n",
        "def export_afinn_to_csv(file_path):\n",
        "    # Initialize Afinn and access the wordlist\n",
        "    afinn = Afinn()\n",
        "\n",
        "    # AFINN wordlist is accessible as a dictionary {word: score}\n",
        "    wordlist = afinn._dict\n",
        "\n",
        "    # Write to CSV\n",
        "    with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Word', 'Score'])  # header row\n",
        "        for word, score in wordlist.items():\n",
        "            writer.writerow([word, score])\n",
        "\n",
        "# Specify the path for your CSV file\n",
        "export_afinn_to_csv('afinn_lexicon.csv')\n"
      ],
      "metadata": {
        "id": "h92J8Dtv7DKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER\n",
        "import csv\n",
        "\n",
        "# Write the VADER lexicon to a CSV file\n",
        "with open('vader_lexicon.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Token', 'Sentiment Score'])\n",
        "    for key, value in vader_lexicon.items():\n",
        "        writer.writerow([key, value])"
      ],
      "metadata": {
        "id": "dw6I2cS86ky5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiwordnet - takes long to run\n",
        "import csv\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to get sentiment data\n",
        "def get_sentiwordnet_data():\n",
        "    data = []\n",
        "    # Iterate over all the synset terms in the WordNet\n",
        "    for synset in wn.all_synsets():\n",
        "        # Get SentiWordNet synset equivalent\n",
        "        senti_synset = swn.senti_synset(synset.name())\n",
        "        if senti_synset:\n",
        "            # Prepare data that includes the synset name, its positive, negative, and objective scores\n",
        "            data.append([\n",
        "                synset.name(),\n",
        "                synset.definition(),\n",
        "                senti_synset.pos_score(),\n",
        "                senti_synset.neg_score(),\n",
        "                senti_synset.obj_score()\n",
        "            ])\n",
        "    return data\n",
        "\n",
        "# Get the data from SentiWordNet\n",
        "senti_data = get_sentiwordnet_data()\n",
        "\n",
        "# Write the data to a CSV file\n",
        "with open('sentiwordnet.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Synset', 'Definition', 'Positive Score', 'Negative Score', 'Objective Score'])\n",
        "    writer.writerows(senti_data)\n",
        "\n",
        "print(\"SentiWordNet data has been written to 'sentiwordnet.csv'.\")\n"
      ],
      "metadata": {
        "id": "qVndsoDK8F5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}